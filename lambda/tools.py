import os
import re
import time
import json
import boto3
import random
import logging
from pathlib import Path
from typing import Optional
from litellm import completion
from litellm import RateLimitError

# Setup logging
logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# Litellm needs region to be set
os.environ["AWS_REGION_NAME"] = os.environ["AWS_REGION"]

# sometimes the model refused to generate content due to internal guardrails
# so this is a custom exception to catch that error
class NoContentGeneratedException(Exception):
    pass

# Canned code to use in case the model generates misformatted code
# so this code will also cause the test to fail so in that sense the overall
# accuracy of the model for this benchmark remain unaffected and this code simply
# helps the harness to move to the next problem in the benchmark

FAILED_RESPONSE = """
import sys

def main():
    input = sys.stdin.read
    data = input().split()
    
    # do nothing, this is a canned response so that the eval for
    # this task can silently fail

    print(data)

if __name__ == "__main__":
    main()
"""

# Regular expression pattern to extract Python code blocks from text
# Matches content between ```python and ``` markers, capturing everything in between
REGEX_FOR_PY_CODE_EXTRACTION: str = r"```python\n(.*?)```"

def _process_task(model_name: str, formatted_prompt: str, inference_params: dict) -> str:
    """
    Runs inference for a prompt using the specified model. Retry after sleep logic is in place
    in case of exceptions.
    
    Args:
        model_name (str): The Amazon Bedrock model id to use, needs to start with "bedrock/"
        formatted_prompt (str): Prompt for inference
        inference_params (Dict): inference parameters such as max_tokens, tempertature and n
    Returns:
        str: The completion generated by the model
    
    Note:
        - Raises exception in case retries are exhausedted
    """
    max_retries: int = 10 # set to a rather higher value for Amazon Nova
    retry_delay: int = 60  # seconds
    print(f"formatted_prompt={formatted_prompt}")
    for attempt in range(max_retries):
        try:
            # run inference
            response = completion(
                    model=model_name,
                    model_id=None,
                    messages=[{"role": "user", "content": formatted_prompt}],
                    max_tokens=inference_params["max_tokens"],
                    temperature=inference_params["temperature"],
                    n=inference_params["n"],
                )
            # Debug: logger.info raw response
            logger.info(f"Raw Response: {response}")
            # check if we received an empty response, for example the model saying something like
            # "The generated text has been blocked by our content filters."
            if response['usage']['completion_tokens'] == 0:
                content = response["choices"][0]["message"]["content"]
                raise NoContentGeneratedException(f"completion tokens is 0, content={content}")
            return response["choices"][0]["message"]["content"]

        except NoContentGeneratedException as e:
            if attempt < max_retries - 1:
                # increase delay with every retry and add some random jitter to the delay
                this_retry_delay = retry_delay * (attempt + 1) + random.randint(1, 10)
                logger.error(f"{e}, task on attempt {attempt + 1}. Waiting {retry_delay} seconds...")
                time.sleep(this_retry_delay)
                continue
            else:
                logger.error(f"max retries exceeded for task")
                raise  # Re-raise the exception if we've exhausted all retries    
        except RateLimitError as e:
            if attempt < max_retries - 1:
                # increase delay with every retry and add some random jitter to the delay
                this_retry_delay = retry_delay * (attempt + 1) + random.randint(1, 10)
                logger.error(f"{e}, task on attempt {attempt + 1}. Waiting {this_retry_delay} seconds...")
                time.sleep(this_retry_delay)
                continue
            else:
                logger.error(f"max retries exceeded for task")
                raise  # Re-raise the exception if we've exhausted all retries
                
        except Exception as e:
            logger.error(f"Unexpected error processing task: {str(e)}")
            raise

def hydrate_prompt(prompt_template: str, values: dict) -> str:
    from jinja2 import Template

    # Create a Jinja2 Template object
    template = Template(prompt_template)

    # Render the template with values
    return template.render(values)

def gen_code(query: str, model_id: str) -> Optional[str]:
    """
    Use this tool only when you need to generate code based on the problem. The input is the Problem Statement. The tool returns code that the customer can use.
    """

    # get the prompt from the Amazon Bedrock prompt management
    # the prompt id to use is retrieved from a dictionary stored as an env var string
    model_id_to_prompt_id_mapping = os.environ.get('MODEL_ID_TO_PROMPT_ID_MAPPING')
    logger.info(f"model_id_to_prompt_id_mapping={model_id_to_prompt_id_mapping}")
    if model_id_to_prompt_id_mapping is not None:
        model_id_to_prompt_id_mapping = json.loads(model_id_to_prompt_id_mapping)
    else:
        return None
    
    # prompt id to use
    prompt_id = model_id_to_prompt_id_mapping.get(model_id)
    logger.info(f"found prompt_id={prompt_id} for model_id={model_id}")
    if prompt_id is not None:        
        bedrock_agent = boto3.client(service_name = "bedrock-agent", region_name = os.environ['AWS_REGION'])
        prompt_info = bedrock_agent.get_prompt(promptIdentifier=prompt_id)
        prompt_template = prompt_info['variants'][0]['templateConfiguration']['text']['text']
        prompt = hydrate_prompt(prompt_template, dict(question=query))

        # inference parameters
        inference_params = prompt_info['variants'][0]['inferenceConfiguration']['text']
        # litellm likes max_tokens not maxTokens
        inference_params["max_tokens"] = inference_params.pop("maxTokens")
        inference_params["n"] = 1
        logger.info(f"model_id={model_id}, inference_params={inference_params}, prompt={prompt}")
    else:
        logger.info(f"no prompt id found for model_id={model_id}")
        return None
    
    bedrock_model_id = f"bedrock/{model_id}"
    generated_text = _process_task(bedrock_model_id, prompt, inference_params)
    return generated_text